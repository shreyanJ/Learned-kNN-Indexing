{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import time\n",
    "\n",
    "N = 1000 # 1000 data points in this training set\n",
    "k = 10 # Each node in the graph will have edges to its 10 nearest neighbors\n",
    "S = 50 # need S-nn graph for soft label empirical distribution over S nearest neighbors, S >> k\n",
    "M = 4 # partition this into 4 parts\n",
    "num_points = 8 # each example has 8 points\n",
    "num_features_per_point = 2 # each point has 2 features\n",
    "\n",
    "folder = '../data/imbalanced_toy/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training data\n",
    "points = pickle.load(open(folder + 'points.pickle', 'rb'))\n",
    "assert len(points) == N\n",
    "distances = pickle.load(open(folder + 'distances.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_distance(med, cluster):\n",
    "    total = 0\n",
    "    for i in cluster:\n",
    "        total += distances[points[i], points[med]]\n",
    "    return total\n",
    "\n",
    "def calculate_medoids_for_partition(partition):\n",
    "    partition_copy = [list(part) for part in partition]\n",
    "    medoids = [part[0] for part in partition_copy]\n",
    "    \n",
    "    for i in range(len(medoids)):\n",
    "        for new_med in partition_copy[i][1:]:\n",
    "            if total_distance(new_med, partition[i]) < total_distance(medoids[i], partition[i]):\n",
    "                medoids[i] = new_med\n",
    "    return medoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the knn graph and partitions\n",
    "G = nx.read_gpickle(folder + 'knn_graph.gpickle')\n",
    "\n",
    "# partition_by_component = True\n",
    "# if partition_by_component:\n",
    "#     partitions = [set(range(250)), set(range(250, 500)), set(range(500, 750)), set(range(750,1000))]\n",
    "# else:\n",
    "#     partitions = nx.read_gpickle(folder + 'graph_partitions.pickle')\n",
    "graph_partition = nx.read_gpickle(folder + 'graph_partitions.pickle')\n",
    "km_partitions = nx.read_gpickle(folder + 'km_partitions.pickle')\n",
    "\n",
    "graph_medoids = calculate_medoids_for_partition(graph_partition)\n",
    "km_medoids = pickle.load(open(folder + 'medoids.pickle'), 'rb')\n",
    "\n",
    "assert N == len(G.nodes())\n",
    "assert M == len(graph_partition)\n",
    "assert M == len(km_partitions)\n",
    "levels = int(np.log2(M))\n",
    "assert M == 2 ** levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flat_X_unsorted(points):\n",
    "    # flatten each training point's feature matrix into a single feature vector\n",
    "    # note that not every collision event may have the same number of particles\n",
    "    num_particles = 0\n",
    "    for point in points:\n",
    "        num_particles = max(num_particles, point.shape[0])\n",
    "    num_readings = points[0].shape[1] # every particle should have the same number (3) of readigns\n",
    "\n",
    "    X = []\n",
    "    for point in points:\n",
    "        feature = np.copy(point)\n",
    "        feature.resize((num_particles * num_readings, ))\n",
    "        X.append(feature)\n",
    "    return np.array(X)\n",
    "\n",
    "def get_flat_X_sorted(points):\n",
    "    # flatten each training point's feature matrix into a single feature vector\n",
    "    # note that not every collision event may have the same number of particles\n",
    "    num_particles = 0\n",
    "    for point in points:\n",
    "        num_particles = max(num_particles, point.shape[0])\n",
    "    num_readings = points[0].shape[1] # every particle should have the same number (3) of readigns\n",
    "\n",
    "    X = []\n",
    "    for point in points:\n",
    "        feature = np.copy(point)\n",
    "        \n",
    "        # sort the point data by the first column\n",
    "        index = feature[:,0].argsort()\n",
    "        feature = feature[index]\n",
    "        \n",
    "        # now flatten the feature and fill in with 0's\n",
    "        feature.resize((num_particles * num_readings, ))\n",
    "        X.append(feature)\n",
    "    return np.array(X)\n",
    "    \n",
    "def get_class_labels(partition):\n",
    "#     if partition_by_component:\n",
    "#         labels = []\n",
    "#         for i in range(250):\n",
    "#             labels.append(0)\n",
    "#         for i in range(250, 500):\n",
    "#             labels.append(1)\n",
    "#         for i in range(500, 750):\n",
    "#             labels.append(2)\n",
    "#         for i in range(750, 1000):\n",
    "#             labels.append(3)\n",
    "#         return np.array(labels)\n",
    "#     else:\n",
    "    # create labels for each node\n",
    "    labels_dict = {}\n",
    "    for i, part in enumerate(partition):\n",
    "        for node in part:\n",
    "            labels_dict[node] = i\n",
    "\n",
    "    labels = []\n",
    "    for i in range(N):\n",
    "        labels.append(labels_dict[i])\n",
    "    return np.array(labels)\n",
    "\n",
    "def get_hierarchical_labels(partition):\n",
    "    # return vector of 0 and 1's where each entry determines next splitting point\n",
    "    labels = get_class_labels(partition)\n",
    "    \n",
    "    Y = []\n",
    "    for label in labels:\n",
    "        hierarchical_label = []\n",
    "        for j in range(levels):\n",
    "            hierarchical_label.append(label // (2 ** (levels - 1 - j)))\n",
    "            label = label % (2 ** (levels - 1 - j))\n",
    "        Y.append(hierarchical_label)\n",
    "    return np.array(Y)\n",
    "\n",
    "def get_soft_labels(partition):\n",
    "    # first get regular labels\n",
    "    labels = get_class_labels(partition)\n",
    "\n",
    "    # turn the labels into soft labels\n",
    "    # for this, the label becomes the empirical distribution of the part that each node's S nearest neighbors belong to\n",
    "    nns = nx.read_gpickle(folder + 'nearest_neighbors.pickle')\n",
    "    Y = []\n",
    "    for i in range(N):\n",
    "        distribution = np.zeros(M)\n",
    "        for n in nns[i]:\n",
    "            distribution[labels[n]] += 1\n",
    "        distribution = np.divide(distribution, np.sum(distribution))\n",
    "        Y.append(distribution)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def lr_train_hierarchical(models, prefix, X, y, level):\n",
    "    if level == levels:\n",
    "        return\n",
    "    \n",
    "    clf = LogisticRegression().fit(X, y[:,level])\n",
    "    models[prefix] = clf\n",
    "    \n",
    "    predictions = clf.predict(X)\n",
    "    \n",
    "    all_zeros = np.ones(len(predictions))\n",
    "    X_left = X[predictions == all_zeros]\n",
    "    y_left = y[predictions == all_zeros]\n",
    "    lr_train_hierarchical(models, prefix + '0', X_left, y_left, level + 1)\n",
    "    \n",
    "    all_ones = np.ones(len(predictions))\n",
    "    X_right = X[predictions == all_ones]\n",
    "    y_right = y[predictions == all_ones]\n",
    "    lr_train_hierarchical(models, prefix + '1', X_right, y_right, level + 1)\n",
    "    \n",
    "def lr_predict(model, x, num_levels=levels):\n",
    "    prediction = []\n",
    "    prefix = ''\n",
    "    while len(prefix) < num_levels:\n",
    "        clf = model[prefix]\n",
    "        label = clf.predict(x)[0]\n",
    "        prefix += str(label)\n",
    "        prediction.append(label)\n",
    "    return np.array(prediction)\n",
    "    \n",
    "def lr_prediction_accuracy(model, X, y):\n",
    "    num_levels = levels # how deep in the tree to check\n",
    "    num_correct = 0\n",
    "    \n",
    "    # check each data point\n",
    "    for i in range(len(X)):\n",
    "        x = X[i:i+1,:]\n",
    "        prediction = predict(model, x, num_levels)\n",
    "                \n",
    "        if (y[i][:num_levels] == prediction).all():\n",
    "            num_correct += 1\n",
    "    \n",
    "    return num_correct / len(X)\n",
    "\n",
    "def lr_predict_best_partition(model, x):\n",
    "    return np.dot(np.power(2, range(levels)[::-1]), lr_predict_label(model, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Medoids Classifier code\n",
    "import ot\n",
    "\n",
    "def ot_distance(x, y):\n",
    "    x = x.reshape(-1, 2)\n",
    "    y = y.reshape(-1, 2)\n",
    "    C = ot.dist(x, y)\n",
    "    return ot.emd2([], [], C)\n",
    "\n",
    "def km_predict_best_partition(medoids, x):\n",
    "    best = 0\n",
    "    distance = ot_distance(x, points[medoids[best]])\n",
    "    for i in range(1, len(medoids)):\n",
    "        new_distance = ot_distance(x, points[medoids[i]])\n",
    "        if new_distance < distance:\n",
    "            distance = new_distance\n",
    "            best = i\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_flat_X_sorted(points)\n",
    "y = get_hierarchical_labels(partition)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_query = # load unseen points for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments - KL Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = graph_partition\n",
    "medoids = graph_medoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Sorted Linear Regression Training\n",
    "lr_sorted_model = {}\n",
    "lr_train_hierarchical(lr_sorted_model, \"\", X, y, 0)\n",
    "\n",
    "# now assess test accuracy\n",
    "print(lr_prediction_accuracy(lr_sorted_model, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground Truth\n",
    "knn_real = []\n",
    "t1 = time.time()\n",
    "for i in range(len(X_query)):\n",
    "    x = X_query[i:i+1, :]\n",
    "    distances = []\n",
    "    for node_id in range(N):\n",
    "        d = ot_distance(x, X[node_id:node_id+1, :])\n",
    "        distances.append((node_id, d))\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    knn_real.append([u[0] for u in distances[:k]])\n",
    "t2 = time.time()\n",
    "\n",
    "print('Time: {}'.format(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr_predict_best_partition' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7d2176bfd49a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Model Evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m for model in [('Log Reg Sorted', lr_predict_best_partition, lr_sorted_model), \n\u001b[0m\u001b[1;32m      3\u001b[0m               ('K-Medoids', km_predict_best_partition, medoids)]:\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lr_predict_best_partition' is not defined"
     ]
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "for model in [('Log Reg Sorted', lr_predict_best_partition, lr_sorted_model), \n",
    "              ('K-Medoids', km_predict_best_partition, medoids)]:\n",
    "    print('Model: {}'.format(model[0]))\n",
    "    \n",
    "    knn_sample = []\n",
    "    t1 = time.time()\n",
    "    for i in range(len(X_test)):\n",
    "        x = X_test[i:i+1, :]\n",
    "        label = model[1](model[2], x)\n",
    "        assigned_partition = partition[label]\n",
    "\n",
    "        distances = []\n",
    "        for node_id in assigned_partition:\n",
    "            d = distance(x, X[node_id:node_id+1, :])\n",
    "            distances.append((node_id, d))\n",
    "        distances.sort(key=lambda x: x[1])\n",
    "        knn_sample.append([u[0] for u in distances[:k]])\n",
    "    t2 = time.time()\n",
    "    \n",
    "    print('Time: {}'.format(t2-t1))\n",
    "    \n",
    "    # evaluate accuracy\n",
    "    percents_captured = []\n",
    "    for i in range(len(knn_sample)):\n",
    "        num_correct = len(np.intersect1d(knn_sample[i], knn_real[i]))\n",
    "        percents_captured.append(num_correct / k)\n",
    "    print('Average Accuracy/Recall: {}'.format(np.mean(percents_captured)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = get_class_labels(partition)\n",
    "color_map = []\n",
    "colors = {0: 'red', 1: 'blue', 2: 'green', 3: 'yellow'}\n",
    "\n",
    "for i in range(N):\n",
    "    color_map.append(colors[labels[i]])\n",
    "\n",
    "nx.draw(G, node_color = color_map)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
