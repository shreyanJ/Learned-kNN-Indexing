{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "N = 1000 # 1000 data points in this training set\n",
    "k = 10 # Each node in the graph will have edges to its 10 nearest neighbors\n",
    "S = 50 # need S-nn graph for soft label empirical distribution over S nearest neighbors, S >> k\n",
    "M = 4 # partition this into 4 parts\n",
    "num_points = 8 # each example has 8 points\n",
    "num_features_per_point = 2 # each point has 2 features\n",
    "\n",
    "folder = '../data/small_toy/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create k-NN Graph, If Necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset of points\n",
    "points = pickle.load(open(folder + 'points.pickle', 'rb'))\n",
    "assert len(points) == N\n",
    "for point in points:\n",
    "    # every point is a 8x2 feature\n",
    "    assert point.shape[0] == num_points\n",
    "    assert point.shape[1] == num_features_per_point\n",
    "\n",
    "# Build the k-nearest neighbor graph from these nodes\n",
    "knn_graph = nx.Graph()\n",
    "for i, point in enumerate(points):\n",
    "    knn_graph.add_node(i, reading=point)\n",
    "\n",
    "# Load the pairwise distances\n",
    "distances = pickle.load(open(folder + 'distances.pickle', 'rb'))\n",
    "assert distances.shape == (N, N)\n",
    "\n",
    "# Compute the k and S nearest neighbors by sorting the distances\n",
    "S_nearest_neighbors = {}\n",
    "for i in range(N):\n",
    "    distances_for_point = list(zip(range(N), distances[i]))\n",
    "    distances_for_point.sort(key=lambda x: x[1])\n",
    "\n",
    "    # Add an edge to each of the closest neighbors, skipping the node itself\n",
    "    for j in range(1, k + 1):\n",
    "        if not knn_graph.has_edge(i, distances_for_point[j][0]):\n",
    "            knn_graph.add_edge(i, distances_for_point[j][0], weight=distances_for_point[j][0])\n",
    "\n",
    "    # Keep track of the exactly S nearest neighbors - need this for soft labels\n",
    "    S_nearest_neighbors[i] = [i]\n",
    "    for j in range(1, S):\n",
    "        S_nearest_neighbors[i].append(distances_for_point[j][0])\n",
    "\n",
    "nx.write_gpickle(knn_graph, folder + 'knn_graph.gpickle')\n",
    "pickle.dump(S_nearest_neighbors, open(folder + 'nearest_neighbors.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition Graph using KL if Necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with iteration at level: 2\n",
      "done with iteration at level: 1\n",
      "done with iteration at level: 1\n"
     ]
    }
   ],
   "source": [
    "from networkx.algorithms import community\n",
    "\n",
    "# import the knn graph\n",
    "G = nx.read_gpickle(folder + 'knn_graph.gpickle')\n",
    "\n",
    "# we use a hierarchical partitioning approach to generate m balanced partitions of the graph\n",
    "# specifically, we choose m to be a power of 2 and repeatedly use Kernighan-Lin to bisect the \n",
    "# graph into 2 approximately equal parts\n",
    "def graph_partition(graph, level):\n",
    "    if level == 0:\n",
    "        return [set(graph.nodes())]\n",
    "    else:\n",
    "        part1, part2 = community.kernighan_lin_bisection(graph, weight='weight')\n",
    "        print(\"done with iteration at level: {}\".format(level))\n",
    "        return graph_partition(graph.subgraph(part1), level - 1) + graph_partition(graph.subgraph(part2), level - 1)\n",
    "\n",
    "# First, partition the knn graph formed by the training set\n",
    "num_levels = int(np.log2(M))\n",
    "assert M == 2 ** num_levels\n",
    "partition = graph_partition(G, num_levels)\n",
    "\n",
    "# check the partition was correct\n",
    "union = []\n",
    "for part in partition:\n",
    "    union += list(part)\n",
    "assert set(union) == set(G.nodes())\n",
    "\n",
    "pickle.dump(partition, open(folder + \"graph_partitions.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition Graph with k-Medioids if Necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the knn graph and partitions\n",
    "G = nx.read_gpickle(folder + 'knn_graph.gpickle')\n",
    "\n",
    "# partition_by_component = True\n",
    "# if partition_by_component:\n",
    "#     partitions = [set(range(250)), set(range(250, 500)), set(range(500, 750)), set(range(750,1000))]\n",
    "# else:\n",
    "#     partitions = nx.read_gpickle(folder + 'graph_partitions.pickle')\n",
    "graph_partition = nx.read_gpickle(folder + 'graph_partitions.pickle')\n",
    "km_partitions = nx.read_gpickle(folder + 'km_partitions.pickle')\n",
    "\n",
    "assert N == len(G.nodes())\n",
    "assert M == len(graph_partition)\n",
    "assert M == len(km_partitions)\n",
    "levels = int(np.log2(M))\n",
    "assert M == 2 ** levels\n",
    "\n",
    "# load the training data\n",
    "points = pickle.load(open(folder + 'points.pickle', 'rb'))\n",
    "assert len(points) == N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flat_X_unsorted():\n",
    "    # flatten each training point's feature matrix into a single feature vector\n",
    "    # note that not every collision event may have the same number of particles\n",
    "    num_particles = 0\n",
    "    for point in points:\n",
    "        num_particles = max(num_particles, point.shape[0])\n",
    "    num_readings = points[0].shape[1] # every particle should have the same number (3) of readigns\n",
    "\n",
    "    X = []\n",
    "    for point in points:\n",
    "        feature = np.copy(point)\n",
    "        feature.resize((num_particles * num_readings, ))\n",
    "        X.append(feature)\n",
    "    return np.array(X)\n",
    "\n",
    "def get_flat_X_sorted():\n",
    "    # flatten each training point's feature matrix into a single feature vector\n",
    "    # note that not every collision event may have the same number of particles\n",
    "    num_particles = 0\n",
    "    for point in points:\n",
    "        num_particles = max(num_particles, point.shape[0])\n",
    "    num_readings = points[0].shape[1] # every particle should have the same number (3) of readigns\n",
    "\n",
    "    X = []\n",
    "    for point in points:\n",
    "        feature = np.copy(point)\n",
    "        \n",
    "        # sort the point data by the first column\n",
    "        index = feature[:,0].argsort()\n",
    "        feature = feature[index]\n",
    "        \n",
    "        # now flatten the feature and fill in with 0's\n",
    "        feature.resize((num_particles * num_readings, ))\n",
    "        X.append(feature)\n",
    "    return np.array(X)\n",
    "    \n",
    "def get_class_labels(partition):\n",
    "#     if partition_by_component:\n",
    "#         labels = []\n",
    "#         for i in range(250):\n",
    "#             labels.append(0)\n",
    "#         for i in range(250, 500):\n",
    "#             labels.append(1)\n",
    "#         for i in range(500, 750):\n",
    "#             labels.append(2)\n",
    "#         for i in range(750, 1000):\n",
    "#             labels.append(3)\n",
    "#         return np.array(labels)\n",
    "#     else:\n",
    "    # create labels for each node\n",
    "    labels_dict = {}\n",
    "    for i, part in enumerate(partition):\n",
    "        for node in part:\n",
    "            labels_dict[node] = i\n",
    "\n",
    "    labels = []\n",
    "    for i in range(N):\n",
    "        labels.append(labels_dict[i])\n",
    "    return np.array(labels)\n",
    "\n",
    "def get_hierarchical_labels(partition):\n",
    "    # return vector of 0 and 1's where each entry determines next splitting point\n",
    "    labels = get_class_labels(partition)\n",
    "    \n",
    "    Y = []\n",
    "    for label in labels:\n",
    "        hierarchical_label = []\n",
    "        for j in range(levels):\n",
    "            hierarchical_label.append(label // (2 ** (levels - 1 - j)))\n",
    "            label = label % (2 ** (levels - 1 - j))\n",
    "        Y.append(hierarchical_label)\n",
    "    return np.array(Y)\n",
    "\n",
    "def get_soft_labels(partition):\n",
    "    # first get regular labels\n",
    "    labels = get_class_labels(partition)\n",
    "\n",
    "    # turn the labels into soft labels\n",
    "    # for this, the label becomes the empirical distribution of the part that each node's S nearest neighbors belong to\n",
    "    nns = nx.read_gpickle(folder + 'nearest_neighbors.pickle')\n",
    "    Y = []\n",
    "    for i in range(N):\n",
    "        distribution = np.zeros(M)\n",
    "        for n in nns[i]:\n",
    "            distribution[labels[n]] += 1\n",
    "        distribution = np.divide(distribution, np.sum(distribution))\n",
    "        Y.append(distribution)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def lr_train_hierarchical(models, prefix, X, y, level):\n",
    "    if level == levels:\n",
    "        return\n",
    "    \n",
    "    clf = LogisticRegression().fit(X, y[:,level])\n",
    "    models[prefix] = clf\n",
    "    \n",
    "    predictions = clf.predict(X)\n",
    "    \n",
    "    all_zeros = np.ones(len(predictions))\n",
    "    X_left = X[predictions == all_zeros]\n",
    "    y_left = y[predictions == all_zeros]\n",
    "    lr_train_hierarchical(models, prefix + '0', X_left, y_left, level + 1)\n",
    "    \n",
    "    all_ones = np.ones(len(predictions))\n",
    "    X_right = X[predictions == all_ones]\n",
    "    y_right = y[predictions == all_ones]\n",
    "    lr_train_hierarchical(models, prefix + '1', X_right, y_right, level + 1)\n",
    "    \n",
    "def lr_predict(model, x, num_levels=levels):\n",
    "    prediction = []\n",
    "    prefix = ''\n",
    "    while len(prefix) < num_levels:\n",
    "        clf = model[prefix]\n",
    "        label = clf.predict(x)[0]\n",
    "        prefix += str(label)\n",
    "        prediction.append(label)\n",
    "    return np.array(prediction)\n",
    "    \n",
    "def lr_prediction_accuracy(model, X, y):\n",
    "    num_levels = levels # how deep in the tree to check\n",
    "    num_correct = 0\n",
    "    \n",
    "    # check each data point\n",
    "    for i in range(len(X)):\n",
    "        x = X[i:i+1,:]\n",
    "        prediction = predict(model, x, num_levels)\n",
    "                \n",
    "        if (y[i][:num_levels] == prediction).all():\n",
    "            num_correct += 1\n",
    "    \n",
    "    return num_correct / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from energyflow.emd import emd, emds\n",
    "import ot\n",
    "\n",
    "def ot_distance(x, y):\n",
    "    x = x.reshape(-1, 2)\n",
    "    y = y.reshape(-1, 2)\n",
    "    C = ot.dist(x, y)\n",
    "    return ot.emd2([], [], C)\n",
    "\n",
    "metric = DistanceMetric.get_metric('pyfunc', func=distance)\n",
    "nb = NearestNeighbors(metric=distance, algorithm='ball_tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments - KL Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = graph_partition\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Sorted Linear Regression\n",
    "X = get_flat_X_sorted()\n",
    "y = get_hierarchical_labels(partition)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# train the top_level_model\n",
    "lr_sorted_model = {}\n",
    "lr_train_hierarchical(lr_sorted_model, \"\", X_train, y_train, 0)\n",
    "\n",
    "# now assess test accuracy\n",
    "print(lr_prediction_accuracy(lr_sorted_model, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Centroid Linear Regression\n",
    "X = #get_flat_X_sorted()\n",
    "y = get_hierarchical_labels(partition)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# train the top_level_model\n",
    "lr_centroid_model = {}\n",
    "lr_train_hierarchical(lr_centroid_model, \"\", X_train, y_train, 0)\n",
    "\n",
    "# now assess test accuracy\n",
    "print(lr_prediction_accuracy(lr_centroid_model, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.835469722747803\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "k = 10\n",
    "knn_sample = []\n",
    "\n",
    "t1 = time.time()\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test[i:i+1, :]\n",
    "    label = predict(x, levels)\n",
    "    assigned_partition = partitions[np.dot(np.power(2, range(levels)[::-1]), label)]\n",
    "    \n",
    "    distances = []\n",
    "    for node_id in assigned_partition:\n",
    "        d = distance(x, X[node_id:node_id+1, :])\n",
    "        distances.append((node_id, d))\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    knn_sample.append([u[0] for u in distances[:k]])\n",
    "t2 = time.time()\n",
    "\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.400413274765015\n"
     ]
    }
   ],
   "source": [
    "knn_real = []\n",
    "t1 = time.time()\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test[i:i+1, :]\n",
    "    distances = []\n",
    "    for node_id in range(N):\n",
    "        d = ot_distance(x, X[node_id:node_id+1, :])\n",
    "        distances.append((node_id, d))\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    knn_real.append([u[0] for u in distances[:k]])\n",
    "t2 = time.time()\n",
    "\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# now evaluate accuracy\n",
    "percents_captured = []\n",
    "for i in range(len(knn_sample)):\n",
    "    num_correct = len(np.intersect1d(knn_sample[i], knn_real[i]))\n",
    "    percents_captured.append(num_correct / k)\n",
    "print(np.mean(percents_captured))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = get_class_labels(partition)\n",
    "color_map = []\n",
    "colors = {0: 'red', 1: 'blue', 2: 'green', 3: 'yellow'}\n",
    "\n",
    "for i in range(N):\n",
    "    color_map.append(colors[labels[i]])\n",
    "\n",
    "nx.draw(G, node_color = color_map)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
